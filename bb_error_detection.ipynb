{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMShvJWE9tf6TazepVxQCKb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ldessemon2/myailearning/blob/main/bb_error_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "fQoklA8QODw9"
      },
      "outputs": [],
      "source": [
        "from google.cloud import bigquery\n",
        "import pandas as pd\n",
        "from datetime import timedelta\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import IsolationForest, RandomForestClassifier\n",
        "from sklearn.svm import OneClassSVM\n",
        "from sklearn.metrics import classification_report, roc_auc_score, roc_curve, precision_recall_curve, average_precision_score\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace with your BigQuery project ID\n",
        "PROJECT_ID = 'internal-naoxyatrainingservice'\n",
        "VIEW_NAME = 'oxyaca.beebole_all_view'"
      ],
      "metadata": {
        "id": "vSiVzyW-OHVU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_timesheet_data(project_id: str, view_name: str) -> pd.DataFrame:\n",
        "    # ... (same as before)\n",
        "    client = bigquery.Client(project=project_id)\n",
        "    query = f\"\"\"\n",
        "        SELECT\n",
        "            person_name,\n",
        "            CAST(hours AS FLOAT64) AS hours,\n",
        "            CAST(billableHours AS FLOAT64) AS billableHours,\n",
        "            project_name,\n",
        "            subproject_name,\n",
        "            task_name,\n",
        "            CAST(nonBillableHours AS FLOAT64) AS nonBillableHours,\n",
        "            CAST(month AS INT64) AS month,\n",
        "            CAST(year AS INT64) AS year,\n",
        "            CAST(week AS INT64) AS week,\n",
        "            # Directly select the 'date' column, assuming it's already a DATE type\n",
        "            date\n",
        "        FROM\n",
        "            `{view_name}`\n",
        "    \"\"\"\n",
        "    df = client.query(query).to_dataframe()\n",
        "    return df"
      ],
      "metadata": {
        "id": "fOsmKqWqOMGy"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_weekly_features(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    # ... (same as before)\n",
        "    weekly_data = df.groupby(['person_name', 'year', 'week']).agg(\n",
        "        total_hours=('hours', 'sum'),\n",
        "        total_billable_hours=('billableHours', 'sum'),\n",
        "        total_non_billable_hours=('nonBillableHours', 'sum'),\n",
        "        distinct_projects=('project_name', 'nunique'),\n",
        "        distinct_tasks=('task_name', 'nunique'),\n",
        "        first_day_of_week=('date', 'min'),\n",
        "        last_day_of_week=('date', 'max')\n",
        "    ).reset_index()\n",
        "\n",
        "    weekly_data['billable_ratio'] = weekly_data['total_billable_hours'] / weekly_data['total_hours']\n",
        "    weekly_data['non_billable_ratio'] = weekly_data['total_non_billable_hours'] / weekly_data['total_hours']\n",
        "    weekly_data.replace([float('inf'), float('-inf'), pd.NA], 0, inplace=True)\n",
        "\n",
        "    def count_days_worked(row):\n",
        "        if row['first_day_of_week'] and row['last_day_of_week']:\n",
        "            return (row['last_day_of_week'] - row['first_day_of_week']).days + 1\n",
        "        return 0\n",
        "\n",
        "    weekly_data['days_worked'] = weekly_data.apply(count_days_worked, axis=1)\n",
        "    weekly_data['avg_hours_per_day'] = weekly_data['total_hours'] / weekly_data['days_worked'].replace(0, 1)\n",
        "\n",
        "    return weekly_data"
      ],
      "metadata": {
        "id": "7CckVK0FOOL3"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def identify_potential_fraud(weekly_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    # ... (same as before)\n",
        "    weekly_df['fraud_flag'] = 0\n",
        "    weekly_df.loc[weekly_df['total_hours'] > 60, 'fraud_flag'] = 1\n",
        "    weekly_df.loc[weekly_df['distinct_projects'] > 5, 'fraud_flag'] = 1\n",
        "    weekly_df.loc[weekly_df['distinct_tasks'] > 10, 'fraud_flag'] = 1\n",
        "    weekly_df.loc[(weekly_df['billable_ratio'] < 0.1) & (weekly_df['total_hours'] > 10), 'fraud_flag'] = 1\n",
        "    weekly_df.loc[(weekly_df['billable_ratio'] > 0.95) & (weekly_df['total_hours'] > 10), 'fraud_flag'] = 1\n",
        "    weekly_df.loc[weekly_df['avg_hours_per_day'] > 12, 'fraud_flag'] = 1\n",
        "    weekly_df.loc[weekly_df['avg_hours_per_day'] < 2, 'fraud_flag'] = 1\n",
        "    return weekly_df"
      ],
      "metadata": {
        "id": "voX14ujjQO1H"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def simple_anomaly_detection(weekly_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    # ... (same as before)\n",
        "    weekly_df['anomaly_flag'] = 0\n",
        "    for employee in weekly_df['person_name'].unique():\n",
        "        employee_data = weekly_df[weekly_df['person_name'] == employee]\n",
        "        mean_hours = employee_data['total_hours'].mean()\n",
        "        std_hours = employee_data['total_hours'].std()\n",
        "        if std_hours > 0:\n",
        "            threshold = 2\n",
        "            weekly_df.loc[\n",
        "                (weekly_df['person_name'] == employee) &\n",
        "                ((weekly_df['total_hours'] - mean_hours) / std_hours > threshold),\n",
        "                'anomaly_flag'\n",
        "            ] = 1\n",
        "            weekly_df.loc[\n",
        "                (weekly_df['person_name'] == employee) &\n",
        "                ((mean_hours - weekly_df['total_hours']) / std_hours > threshold),\n",
        "                'anomaly_flag'\n",
        "            ] = 1\n",
        "    return weekly_df\n"
      ],
      "metadata": {
        "id": "mdAPgwGOQRED"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def advanced_feature_engineering(weekly_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Creates more advanced features for fraud detection.\"\"\"\n",
        "\n",
        "    # Sort data by employee and week\n",
        "    weekly_df = weekly_df.sort_values(by=['person_name', 'year', 'week'])\n",
        "\n",
        "    # 1. Rate of change in total hours compared to the previous week\n",
        "    weekly_df['prev_week_total_hours'] = weekly_df.groupby('person_name')['total_hours'].shift(1)\n",
        "    weekly_df['hours_change_rate'] = (weekly_df['total_hours'] - weekly_df['prev_week_total_hours']) / weekly_df['prev_week_total_hours']\n",
        "    weekly_df['hours_change_rate'].replace([float('inf'), float('-inf'), pd.NA], 0, inplace=True)\n",
        "\n",
        "    # 2. Rate of change in number of distinct projects\n",
        "    weekly_df['prev_week_distinct_projects'] = weekly_df.groupby('person_name')['distinct_projects'].shift(1)\n",
        "    weekly_df['project_change_rate'] = (weekly_df['distinct_projects'] - weekly_df['prev_week_distinct_projects']) / weekly_df['prev_week_distinct_projects']\n",
        "    weekly_df['project_change_rate'].replace([float('inf'), float('-inf'), pd.NA], 0, inplace=True)\n",
        "\n",
        "    # 3. Ratio of billable hours to total hours (already calculated but can be used)\n",
        "\n",
        "    # 4. Average hours worked on the same project within a week\n",
        "    def avg_hours_per_project(group):\n",
        "        total_hours_worked = group['hours'].sum()\n",
        "        num_projects = group['project_name'].nunique()\n",
        "        return total_hours_worked / num_projects if num_projects > 0 else 0\n",
        "\n",
        "    project_avg_hours = timesheet_df.groupby(['person_name', 'year', 'week', 'project_name']).agg(\n",
        "        hours=('hours', 'sum')\n",
        "    ).reset_index()\n",
        "    project_avg_hours['avg_project_hours'] = project_avg_hours.groupby(['person_name', 'year', 'week'])['hours'].transform(lambda x: x.sum() / x.count() if x.count() > 0 else 0)\n",
        "    weekly_df = pd.merge(weekly_df, project_avg_hours[['person_name', 'year', 'week', 'avg_project_hours']].drop_duplicates(subset=['person_name', 'year', 'week']), on=['person_name', 'year', 'week'], how='left')\n",
        "\n",
        "    # 5. Flag for working on an unusually high number of projects in a single day (requires daily granularity)\n",
        "    # ----> Changed Code Block <----\n",
        "    # Merge timesheet_df with weekly_df to get year and week info for daily data\n",
        "    daily_data_with_week_info = pd.merge(timesheet_df[['person_name', 'date', 'year', 'week', 'project_name']], weekly_df[['person_name', 'year', 'week']].drop_duplicates(), on=['person_name', 'year', 'week'], how='inner')\n",
        "\n",
        "    daily_project_counts = daily_data_with_week_info.groupby(['person_name', 'date'])['project_name'].nunique().reset_index(name='daily_project_count')\n",
        "\n",
        "    # Now group by person_name, year, and week\n",
        "    weekly_df = pd.merge(weekly_df, daily_project_counts.groupby(['person_name', 'year', 'week'])['daily_project_count'].max().reset_index(name='max_daily_projects'), on=['person_name', 'year', 'week'], how='left')\n",
        "\n",
        "    return weekly_df"
      ],
      "metadata": {
        "id": "73tyrR4dQUut"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_anomaly_detection_model(weekly_features_df: pd.DataFrame):\n",
        "    \"\"\"Trains an anomaly detection model (Isolation Forest).\"\"\"\n",
        "    # Select features for the model (exclude non-numeric or identifier columns)\n",
        "    features = ['total_hours', 'total_billable_hours', 'total_non_billable_hours',\n",
        "                'distinct_projects', 'distinct_tasks', 'billable_ratio',\n",
        "                'non_billable_ratio', 'days_worked', 'avg_hours_per_day',\n",
        "                'hours_change_rate', 'project_change_rate', 'avg_project_hours', 'max_daily_projects']\n",
        "    X = weekly_features_df[features].fillna(weekly_features_df[features].mean()) # Handle missing values\n",
        "\n",
        "    # Train Isolation Forest model\n",
        "    model = IsolationForest(contamination='auto', random_state=42)\n",
        "    model.fit(X)\n",
        "\n",
        "    # Get anomaly scores and predictions (-1 for anomaly, 1 for inlier)\n",
        "    weekly_features_df['anomaly_score'] = model.decision_function(X)\n",
        "    weekly_features_df['anomaly_prediction'] = model.predict(X)\n",
        "    weekly_features_df['is_anomaly'] = weekly_features_df['anomaly_prediction'].apply(lambda x: 1 if x == -1 else 0)\n",
        "\n",
        "    return weekly_features_df, model"
      ],
      "metadata": {
        "id": "6EfUtWYfQW4e"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_fraud_classification_model(weekly_features_df: pd.DataFrame, labels: pd.Series):\n",
        "    \"\"\"Trains a fraud classification model (Random Forest).\n",
        "\n",
        "    Args:\n",
        "        weekly_features_df: DataFrame with weekly features.\n",
        "        labels: Pandas Series containing the fraud labels (1 for fraud, 0 for not fraud).\n",
        "    \"\"\"\n",
        "    features = ['total_hours', 'total_billable_hours', 'total_non_billable_hours',\n",
        "                'distinct_projects', 'distinct_tasks', 'billable_ratio',\n",
        "                'non_billable_ratio', 'days_worked', 'avg_hours_per_day',\n",
        "                'hours_change_rate', 'project_change_rate', 'avg_project_hours', 'max_daily_projects']\n",
        "    X = weekly_features_df[features].fillna(weekly_features_df[features].mean())\n",
        "    y = labels\n",
        "\n",
        "    # Split data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "    # Train Random Forest classifier\n",
        "    model = RandomForestClassifier(random_state=42, class_weight='balanced') # Consider class imbalance\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions on the test set\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_proba = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    print(\"\\nFraud Classification Model Evaluation:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    print(f\"AUC: {roc_auc_score(y_test, y_proba)}\")\n",
        "\n",
        "    # Plot ROC Curve\n",
        "    fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc_score(y_test, y_proba):.2f})')\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver Operating Characteristic')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "\n",
        "    # Plot Precision-Recall Curve\n",
        "    precision, recall, _ = precision_recall_curve(y_test, y_proba)\n",
        "    average_precision = average_precision_score(y_test, y_proba)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(recall, precision, color='blue', lw=2, label=f'Precision-Recall curve (AP = {average_precision:.2f})')\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.title('Precision-Recall Curve')\n",
        "    plt.legend(loc=\"lower left\")\n",
        "    plt.show()\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "OttxEb5dQacF"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def deploy_model(model, weekly_data: pd.DataFrame, features: list):\n",
        "    \"\"\"Illustrative function for deploying the model (can be adapted).\"\"\"\n",
        "    X_new = weekly_data[features].fillna(weekly_data[features].mean())\n",
        "    if hasattr(model, 'predict_proba'):\n",
        "        predictions = model.predict_proba(X_new)[:, 1] # Get probability of fraud\n",
        "    else:\n",
        "        predictions = model.decision_function(X_new) # Get anomaly scores\n",
        "\n",
        "    # You would typically integrate this with a system to flag or review these predictions\n",
        "    weekly_data['fraud_probability'] = predictions\n",
        "    potential_fraud = weekly_data[weekly_data['fraud_probability'] > 0.7] # Example threshold\n",
        "\n",
        "    print(\"\\nPotential Fraudulent Weeks (based on deployed model):\")\n",
        "    print(potential_fraud[['person_name', 'year', 'week', 'fraud_probability']])"
      ],
      "metadata": {
        "id": "QMcxgNRnQdFW"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "NpCzIuK4Q7ki"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Fetch data from BigQuery\n",
        "print(\"Fetching data from BigQuery...\")\n",
        "timesheet_df = fetch_timesheet_data(PROJECT_ID, VIEW_NAME)\n",
        "print(\"Data fetched successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjCf-hp0Qgi_",
        "outputId": "2e289833-f7a8-46d9-966e-3548633cc805"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching data from BigQuery...\n",
            "Data fetched successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Calculate weekly features\n",
        "print(\"Calculating weekly features...\")\n",
        "weekly_features_df = calculate_weekly_features(timesheet_df)\n",
        "print(\"Weekly features calculated.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2amEXCG0W0aJ",
        "outputId": "b7ccc96f-3875-4b76-ee51-41d7b5b5af11"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating weekly features...\n",
            "Weekly features calculated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Identify potential fraud using rule-based approach\n",
        "print(\"Identifying potential fraud using rule-based approach...\")\n",
        "fraud_identified_df = identify_potential_fraud(weekly_features_df.copy())\n",
        "potential_fraud_weeks_rule_based = fraud_identified_df[fraud_identified_df['fraud_flag'] == 1]\n",
        "if not potential_fraud_weeks_rule_based.empty:\n",
        "    print(\"\\nPotential fraudulent weekly timesheets (Rule-based):\")\n",
        "    print(potential_fraud_weeks_rule_based)\n",
        "else:\n",
        "    print(\"\\nNo potential fraudulent weekly timesheets found based on the defined rules.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3EHA8rsuW5wU",
        "outputId": "6c630057-f003-407c-ffb9-8fa3dc4d0c07"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Identifying potential fraud using rule-based approach...\n",
            "\n",
            "Potential fraudulent weekly timesheets (Rule-based):\n",
            "             person_name  year  week  total_hours  total_billable_hours  \\\n",
            "1      Abdelkader Kechad  2021    13         40.0                  30.5   \n",
            "3      Abdelkader Kechad  2021    15         41.5                  36.5   \n",
            "4      Abdelkader Kechad  2021    16         40.0                  34.0   \n",
            "5      Abdelkader Kechad  2021    17         40.0                  26.0   \n",
            "6      Abdelkader Kechad  2021    18         40.0                  34.5   \n",
            "...                  ...   ...   ...          ...                   ...   \n",
            "11451       Yazid Hocine  2024    14         40.0                  34.5   \n",
            "11452       Yazid Hocine  2024    15         40.0                  34.5   \n",
            "11453       Yazid Hocine  2024    16         40.0                  33.5   \n",
            "11454       Yazid Hocine  2024    17         41.5                  33.0   \n",
            "11455       Yazid Hocine  2024    18         40.0                  35.0   \n",
            "\n",
            "       total_non_billable_hours  distinct_projects  distinct_tasks  \\\n",
            "1                           9.5                  6               3   \n",
            "3                           5.0                  6               4   \n",
            "4                           6.0                  8               4   \n",
            "5                          14.0                  8               4   \n",
            "6                           5.5                  8               4   \n",
            "...                         ...                ...             ...   \n",
            "11451                       5.5                  6               2   \n",
            "11452                       5.5                  8               4   \n",
            "11453                       6.5                  9               3   \n",
            "11454                       8.5                  6               4   \n",
            "11455                       5.0                  9               4   \n",
            "\n",
            "      first_day_of_week last_day_of_week  billable_ratio  non_billable_ratio  \\\n",
            "1            2021-03-29       2021-04-02        0.762500            0.237500   \n",
            "3            2021-04-12       2021-04-16        0.879518            0.120482   \n",
            "4            2021-04-19       2021-04-23        0.850000            0.150000   \n",
            "5            2021-04-26       2021-04-30        0.650000            0.350000   \n",
            "6            2021-05-03       2021-05-07        0.862500            0.137500   \n",
            "...                 ...              ...             ...                 ...   \n",
            "11451        2024-04-08       2024-04-12        0.862500            0.137500   \n",
            "11452        2024-04-15       2024-04-19        0.862500            0.137500   \n",
            "11453        2024-04-22       2024-04-26        0.837500            0.162500   \n",
            "11454        2024-04-28       2024-05-03        0.795181            0.204819   \n",
            "11455        2024-05-06       2024-05-10        0.875000            0.125000   \n",
            "\n",
            "       days_worked  avg_hours_per_day  fraud_flag  \n",
            "1                5           8.000000           1  \n",
            "3                5           8.300000           1  \n",
            "4                5           8.000000           1  \n",
            "5                5           8.000000           1  \n",
            "6                5           8.000000           1  \n",
            "...            ...                ...         ...  \n",
            "11451            5           8.000000           1  \n",
            "11452            5           8.000000           1  \n",
            "11453            5           8.000000           1  \n",
            "11454            6           6.916667           1  \n",
            "11455            5           8.000000           1  \n",
            "\n",
            "[8190 rows x 15 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Implement a simple anomaly detection based on total hours\n",
        "print(\"\\nPerforming simple anomaly detection based on total hours...\")\n",
        "anomaly_detected_df_simple = simple_anomaly_detection(weekly_features_df.copy())\n",
        "anomalous_weeks_simple = anomaly_detected_df_simple[anomaly_detected_df_simple['anomaly_flag'] == 1]\n",
        "if not anomalous_weeks_simple.empty:\n",
        "    print(\"\\nAnomalous weekly timesheets (Simple Anomaly Detection):\")\n",
        "    print(anomalous_weeks_simple)\n",
        "else:\n",
        "    print(\"\\nNo anomalous weekly timesheets found based on the simple anomaly detection.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZlPb3CvbXFeV",
        "outputId": "c01fc6c4-a1f6-43ba-a224-6cc4f02652e0"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Performing simple anomaly detection based on total hours...\n",
            "\n",
            "Anomalous weekly timesheets (Simple Anomaly Detection):\n",
            "             person_name  year  week  total_hours  total_billable_hours  \\\n",
            "60     Abdelkader Kechad  2022    20         60.5                  59.0   \n",
            "61     Abdelkader Kechad  2022    21         53.0                  44.0   \n",
            "108    Abdelkader Kechad  2023    16         55.0                  53.0   \n",
            "115    Abdelkader Kechad  2023    23         55.0                  54.0   \n",
            "118    Abdelkader Kechad  2023    26         53.0                  43.0   \n",
            "...                  ...   ...   ...          ...                   ...   \n",
            "11416       Yazid Hocine  2023    32         46.5                  44.5   \n",
            "11417       Yazid Hocine  2023    33         45.0                  31.0   \n",
            "11425       Yazid Hocine  2023    41         54.5                  45.5   \n",
            "11427       Yazid Hocine  2023    43         48.0                  48.0   \n",
            "11450       Yazid Hocine  2024    13         48.5                  32.5   \n",
            "\n",
            "       total_non_billable_hours  distinct_projects  distinct_tasks  \\\n",
            "60                          1.5                  5               5   \n",
            "61                          9.0                  5               4   \n",
            "108                         2.0                  4               4   \n",
            "115                         1.0                  4               3   \n",
            "118                        10.0                  4               4   \n",
            "...                         ...                ...             ...   \n",
            "11416                       2.0                  8               3   \n",
            "11417                      14.0                  6               3   \n",
            "11425                       9.0                  6               3   \n",
            "11427                       0.0                  7               2   \n",
            "11450                      16.0                  5               1   \n",
            "\n",
            "      first_day_of_week last_day_of_week  billable_ratio  non_billable_ratio  \\\n",
            "60           2022-05-16       2022-05-21        0.975207            0.024793   \n",
            "61           2022-05-22       2022-05-27        0.830189            0.169811   \n",
            "108          2023-04-16       2023-04-21        0.963636            0.036364   \n",
            "115          2023-06-05       2023-06-10        0.981818            0.018182   \n",
            "118          2023-06-25       2023-07-01        0.811321            0.188679   \n",
            "...                 ...              ...             ...                 ...   \n",
            "11416        2023-08-07       2023-08-11        0.956989            0.043011   \n",
            "11417        2023-08-14       2023-08-18        0.688889            0.311111   \n",
            "11425        2023-10-09       2023-10-13        0.834862            0.165138   \n",
            "11427        2023-10-23       2023-10-28        1.000000            0.000000   \n",
            "11450        2024-04-01       2024-04-06        0.670103            0.329897   \n",
            "\n",
            "       days_worked  avg_hours_per_day  anomaly_flag  \n",
            "60               6          10.083333             1  \n",
            "61               6           8.833333             1  \n",
            "108              6           9.166667             1  \n",
            "115              6           9.166667             1  \n",
            "118              7           7.571429             1  \n",
            "...            ...                ...           ...  \n",
            "11416            5           9.300000             1  \n",
            "11417            5           9.000000             1  \n",
            "11425            5          10.900000             1  \n",
            "11427            6           8.000000             1  \n",
            "11450            6           8.083333             1  \n",
            "\n",
            "[517 rows x 15 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Advanced Feature Engineering\n",
        "print(\"\\nPerforming advanced feature engineering...\")\n",
        "weekly_features_df_advanced = advanced_feature_engineering(weekly_features_df.copy())\n",
        "print(\"Advanced features engineered.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 670
        },
        "id": "bdtbB7fTXPCl",
        "outputId": "2300aed9-6a73-4152-bcfc-7ed31cf9faac"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Performing advanced feature engineering...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-26-1aee02e7ecb5>:10: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  weekly_df['hours_change_rate'].replace([float('inf'), float('-inf'), pd.NA], 0, inplace=True)\n",
            "<ipython-input-26-1aee02e7ecb5>:15: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  weekly_df['project_change_rate'].replace([float('inf'), float('-inf'), pd.NA], 0, inplace=True)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'year'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-1c4c7bef6556>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 5. Advanced Feature Engineering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nPerforming advanced feature engineering...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mweekly_features_df_advanced\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madvanced_feature_engineering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweekly_features_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Advanced features engineered.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-1aee02e7ecb5>\u001b[0m in \u001b[0;36madvanced_feature_engineering\u001b[0;34m(weekly_df)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;31m# Now group by person_name, year, and week\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mweekly_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweekly_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdaily_project_counts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'person_name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'year'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'week'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'daily_project_count'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'max_daily_projects'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'person_name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'year'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'week'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mweekly_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mgroupby\u001b[0;34m(self, by, axis, level, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[1;32m   9181\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You have to supply one of 'by' and 'level'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 9183\u001b[0;31m         return DataFrameGroupBy(\n\u001b[0m\u001b[1;32m   9184\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9185\u001b[0m             \u001b[0mkeys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[1;32m   1327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgrouper\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m             grouper, exclusions, obj = get_grouper(\n\u001b[0m\u001b[1;32m   1330\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m                 \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/groupby/grouper.py\u001b[0m in \u001b[0;36mget_grouper\u001b[0;34m(obj, key, axis, level, sort, observed, validate, dropna)\u001b[0m\n\u001b[1;32m   1041\u001b[0m                 \u001b[0min_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1043\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1044\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGrouper\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mgpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m             \u001b[0;31m# Add key to exclusions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'year'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Model Selection and Training (Example: Anomaly Detection with Isolation Forest)\n",
        "    print(\"\\nTraining Anomaly Detection Model (Isolation Forest)...\")\n",
        "    weekly_features_df_with_anomalies, anomaly_model = train_anomaly_detection_model(weekly_features_df_advanced.copy())\n",
        "    anomalous_weeks_advanced = weekly_features_df_with_anomalies[weekly_features_df_with_anomalies['is_anomaly'] == 1]\n",
        "    if not anomalous_weeks_advanced.empty:\n",
        "        print(\"\\nAnomalous weekly timesheets (Isolation Forest):\")\n",
        "        print(anomalous_weeks_advanced[['person_name', 'year', 'week', 'anomaly_score', 'anomaly_prediction']])\n",
        "    else:\n",
        "        print(\"\\nNo anomalies detected by the Isolation Forest model.\")"
      ],
      "metadata": {
        "id": "Y2yQYOv1bATg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Model Selection and Training (Example: Fraud Classification with Random Forest - Requires Labeled Data)\n",
        "    # **Important:** To run this part, you need a column in your `weekly_features_df_advanced` that indicates whether a week was actually fraudulent (e.g., a 'is_fraud' column with 0 or 1).\n",
        "    if 'is_fraud' in weekly_features_df_advanced.columns:\n",
        "        print(\"\\nTraining Fraud Classification Model (Random Forest)...\")\n",
        "        fraud_model = train_fraud_classification_model(weekly_features_df_advanced.copy(), weekly_features_df_advanced['is_fraud'])\n",
        "\n",
        "        # 8. Deployment (Illustrative)\n",
        "        print(\"\\nIllustrative Deployment of Fraud Classification Model...\")\n",
        "        features_for_model = ['total_hours', 'total_billable_hours', 'total_non_billable_hours',\n",
        "                                'distinct_projects', 'distinct_tasks', 'billable_ratio',\n",
        "                                'non_billable_ratio', 'days_worked', 'avg_hours_per_day',\n",
        "                                'hours_change_rate', 'project_change_rate', 'avg_project_hours', 'max_daily_projects']\n",
        "        deploy_model(fraud_model, weekly_features_df_advanced.copy(), features_for_model)\n",
        "    else:\n",
        "        print(\"\\nSkipping Fraud Classification Model training as no 'is_fraud' label column found.\")\n",
        "\n",
        "    print(\"\\nError fraud detection process completed with advanced steps.\")"
      ],
      "metadata": {
        "id": "6rZpqjczbA5Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}